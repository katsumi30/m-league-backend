import requests
from bs4 import BeautifulSoup
import pandas as pd
import sqlite3
import re

DB_NAME = 'm_league.db'

def get_soup(url):
    print(f"ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url} ...")
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(url, headers=headers)
        res.raise_for_status()
        return BeautifulSoup(res.text, 'html.parser')
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# -------------------------------------------------------
# 1. ãƒãƒ¼ãƒ é †ä½
# -------------------------------------------------------
def scrape_points(conn):
    soup = get_soup("https://m-league.jp/points/")
    if not soup: return
    data = []
    rows = soup.find_all('tr')
    for row in rows:
        try:
            rank_elem = row.find(class_=re.compile('ranking-no'))
            if not rank_elem:
                cols = row.find_all('td')
                if len(cols) >= 3:
                    try:
                        rank = int(cols[0].get_text(strip=True))
                        name = cols[1].get_text(strip=True)
                        point = float(cols[2].get_text(strip=True).replace('pt', '').replace('â–²', '-').replace(',', ''))
                        data.append({"rank": rank, "team": name, "point": point})
                        continue
                    except: pass
            rank = row.find(class_=re.compile('rank-number')).get_text(strip=True)
            name = row.find(class_='team-name').get_text(strip=True)
            point = float(row.find(class_='point').get_text(strip=True).replace('pt', '').replace('â–²', '-').replace(',', ''))
            data.append({"rank": int(rank), "team": name, "point": point})
        except: continue
    
    if not data:
        soup_top = get_soup("https://m-league.jp/")
        if soup_top:
            teams = soup_top.find_all('div', class_='p-ranking__team-item')
            for team in teams:
                try:
                    rank = team.find(class_=re.compile('p-ranking__rank-number')).get_text(strip=True)
                    name = team.find(class_='p-ranking__team-name').get_text(strip=True)
                    point = float(team.find(class_='p-ranking__current-point').get_text(strip=True).replace('pt', '').replace('â–²', '-').replace(',', ''))
                    data.append({"rank": int(rank), "team": name, "point": point})
                except: continue

    if data:
        df = pd.DataFrame(data)
        df.to_sql('team_ranking', conn, if_exists='replace', index=False)
        print(f"âœ… ãƒãƒ¼ãƒ é †ä½: {len(df)} ãƒãƒ¼ãƒ ")

# -------------------------------------------------------
# 2. è©¦åˆçµæœ (â˜…ã“ã“ã‚’ä¿®æ­£ï¼é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’å‰Šé™¤)
# -------------------------------------------------------
def scrape_games(conn):
    base_url = "https://m-league.jp/games/"
    soup = get_soup(base_url)
    if not soup: return

    all_games = []
    
    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã®ã‚»ãƒƒãƒˆã‚’å‰Šé™¤ã—ã¾ã—ãŸ
    # å…¬å¼ã‚µã‚¤ãƒˆã®ãƒ¢ãƒ¼ãƒ€ãƒ«ã¯å…¨ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªIDã‚’æŒã£ã¦ã„ã‚‹ãŸã‚ã€å˜ç´”ã«å…¨éƒ¨å–å¾—ã™ã‚Œã°OKã§ã™

    modals = soup.find_all('div', class_='c-modal2')
    for modal in modals:
        try:
            date_text = modal.find('div', class_='p-gamesResult__date').get_text(strip=True)
            month_day = date_text.split('(')[0]
            parts = month_day.split('/')
            if len(parts) == 2:
                date_str = f"2025/{int(parts[0]):02d}/{int(parts[1]):02d}"
            else:
                date_str = f"2025/{month_day}"

            columns = modal.find_all('div', class_='p-gamesResult__column')
            for col in columns:
                game_num = col.find('div', class_='p-gamesResult__number').get_text(strip=True)
                
                # â˜…å‰Šé™¤ã—ãŸéƒ¨åˆ†: if unique_key in processed_dates: continue

                rank_items = col.find_all('div', class_='p-gamesResult__rank-item')
                for item in rank_items:
                    rank = item.find('div', class_='p-gamesResult__rank-badge').get_text(strip=True)
                    player = item.find('div', class_='p-gamesResult__name').get_text(strip=True).replace(" ", "").replace("ã€€", "")
                    point = float(item.find('div', class_='p-gamesResult__point').get_text(strip=True).replace('pt', '').replace('â–²', '-').replace(',', ''))
                    all_games.append({"date": date_str, "game_count": game_num, "rank": int(rank), "player": player, "point": point})
        except: continue

    if all_games:
        df = pd.DataFrame(all_games)
        # ã‚½ãƒ¼ãƒˆã—ã¦ä¿å­˜
        df = df.sort_values(by=['date', 'game_count'])
        df.to_sql('games', conn, if_exists='replace', index=False)
        print(f"âœ… è©¦åˆçµæœ: {len(df)} ä»¶ä¿å­˜å®Œäº†")
        print(f"   ğŸ“… ãƒ‡ãƒ¼ã‚¿æœŸé–“: {df['date'].min()} ã€œ {df['date'].max()}")
    else:
        print("âš ï¸ è©¦åˆçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# -------------------------------------------------------
# 3. å€‹äººæˆç¸¾
# -------------------------------------------------------
def scrape_stats(conn):
    soup = get_soup("https://m-league.jp/stats/")
    if not soup: return
    data_list = []
    sections = soup.find_all('section', class_='p-stats__team')
    for section in sections:
        try:
            team_name = section.find('h2', class_='p-stats__teamName').get_text(strip=True)
            table = section.find('table', class_='p-stats__table')
            if not table: continue
            rows = table.find_all('tr')
            players = [p.get_text(strip=True).replace(" ", "").replace("ã€€", "") for p in rows[0].find_all('th')[1:]]
            player_stats = {p: {'team': team_name, 'player': p} for p in players}
            key_map = {'è©¦åˆæ•°': 'matches', 'ç·å±€æ•°': 'total_hands', 'ãƒã‚¤ãƒ³ãƒˆ': 'points', 'å¹³ç€': 'avg_rank', '1ä½': 'rank_1_count', '2ä½': 'rank_2_count', '3ä½': 'rank_3_count', '4ä½': 'rank_4_count', 'ãƒˆãƒƒãƒ—ç‡': 'top_rate', 'é€£å¯¾ç‡': 'rentai_rate', 'ãƒ©ã‚¹å›é¿ç‡': 'last_avoid_rate', 'ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢': 'best_score', 'å¹³å‡æ‰“ç‚¹': 'avg_score', 'å‰¯éœ²ç‡': 'furo_rate', 'ãƒªãƒ¼ãƒç‡': 'riichi_rate', 'ã‚¢ã‚¬ãƒªç‡': 'agari_rate', 'æ”¾éŠƒç‡': 'hoju_rate', 'æ”¾éŠƒå¹³å‡æ‰“ç‚¹': 'hoju_avg_score'}
            for row in rows[1:]:
                header = row.find('th').get_text(strip=True)
                if header in key_map:
                    db_key = key_map[header]
                    cols = row.find_all('td')
                    for i, col in enumerate(cols):
                        if i < len(players):
                            val = col.get_text(strip=True)
                            try: val = float(val) if '.' in val else int(val)
                            except: pass
                            player_stats[players[i]][db_key] = val
            data_list.extend(player_stats.values())
        except: continue
    if data_list:
        df = pd.DataFrame(data_list)
        df.to_sql('stats', conn, if_exists='replace', index=False)
        print(f"âœ… å€‹äººã‚¹ã‚¿ãƒƒãƒ„: {len(df)} å")

if __name__ == "__main__":
    conn = sqlite3.connect(DB_NAME)
    print("--- ãƒ‡ãƒ¼ã‚¿å…¨å›åé–‹å§‹ï¼ˆé‡è¤‡è¨±å®¹ç‰ˆï¼‰ ---")
    scrape_points(conn)
    scrape_games(conn)
    scrape_stats(conn)
    conn.close()
    print("--- å®Œäº† ---")