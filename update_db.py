import requests
from bs4 import BeautifulSoup
import pandas as pd
import sqlite3
import re
import time

DB_NAME = 'm_league.db'

def get_soup(url):
    print(f"ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url} ...")
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(url, headers=headers)
        res.raise_for_status()
        return BeautifulSoup(res.text, 'html.parser')
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# -------------------------------------------------------
# 1. ãƒãƒ¼ãƒ é †ä½ (points)
# -------------------------------------------------------
def scrape_points(conn):
    soup = get_soup("https://m-league.jp/points/")
    if not soup: return

    data = []
    rows = soup.find_all('tr')
    for row in rows:
        try:
            rank_elem = row.find(class_=re.compile('ranking-no'))
            if not rank_elem:
                cols = row.find_all('td')
                if len(cols) >= 3:
                    try:
                        rank = int(cols[0].get_text(strip=True))
                        name = cols[1].get_text(strip=True)
                        point = float(cols[2].get_text(strip=True).replace('pt', '').replace('â–²', '-').replace(',', ''))
                        data.append({"rank": rank, "team": name, "point": point})
                        continue
                    except: pass
            rank = row.find(class_=re.compile('rank-number')).get_text(strip=True)
            name = row.find(class_='team-name').get_text(strip=True)
            point = float(row.find(class_='point').get_text(strip=True).replace('pt', '').replace('â–²', '-').replace(',', ''))
            data.append({"rank": int(rank), "team": name, "point": point})
        except: continue
    
    if data:
        df = pd.DataFrame(data)
        df.to_sql('team_ranking', conn, if_exists='replace', index=False)
        print(f"âœ… ãƒãƒ¼ãƒ é †ä½: {len(df)} ãƒãƒ¼ãƒ ä¿å­˜å®Œäº†")

# -------------------------------------------------------
# 2. è©¦åˆçµæœ (éå»ãƒ­ã‚°å·¡å›æ©Ÿèƒ½ä»˜ã) â˜…ã“ã“ã‚’å¤§æ”¹é€ ï¼
# -------------------------------------------------------
def scrape_games(conn):
    # ã¾ãšæœ€æ–°ãƒšãƒ¼ã‚¸ã‚’å–å¾—
    base_url = "https://m-league.jp/games/"
    soup = get_soup(base_url)
    if not soup: return

    all_games = []
    
    # ãƒšãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆéå»ã®è©¦åˆã¸ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™ï¼‰
    # å…¬å¼ã‚µã‚¤ãƒˆã®æ§‹é€ ã«åˆã‚ã›ã¦ã€ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚„ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒªãƒ³ã‚¯ã‚’æ¢ã™
    urls_to_scrape = [base_url]
    
    # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒªãƒ³ã‚¯ï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰ã‚’å–å¾—ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯
    # â€»ç¾åœ¨ã®å…¬å¼ã‚µã‚¤ãƒˆã®æ§‹é€ ä¸Šã€å…¨ã¦ã®ãƒ¢ãƒ¼ãƒ€ãƒ«ãŒ1ãƒšãƒ¼ã‚¸ã«ã‚ã‚‹å ´åˆã¨ã€åˆ†ã‹ã‚Œã¦ã„ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
    # ã“ã“ã§ã¯å¿µã®ãŸã‚ã€è¦‹ãˆã¦ã„ã‚‹ç¯„å›²ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’å¾¹åº•çš„ã«è¡Œã„ã¾ã™ã€‚

    processed_dates = set()

    def extract_from_soup(soup_obj):
        count = 0
        modals = soup_obj.find_all('div', class_='c-modal2')
        for modal in modals:
            try:
                date_text = modal.find('div', class_='p-gamesResult__date').get_text(strip=True)
                # æ—¥ä»˜å‡¦ç†: "9/30" -> "2025/09/30"
                month_day = date_text.split('(')[0]
                parts = month_day.split('/')
                if len(parts) == 2:
                    # ã‚¼ãƒ­åŸ‹ã‚
                    date_str = f"2025/{int(parts[0]):02d}/{int(parts[1]):02d}"
                else:
                    date_str = f"2025/{month_day}"

                columns = modal.find_all('div', class_='p-gamesResult__column')
                for col in columns:
                    game_num = col.find('div', class_='p-gamesResult__number').get_text(strip=True)
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŒã˜è©¦åˆã‚’ä½•åº¦ã‚‚ç™»éŒ²ã—ãªã„ã‚ˆã†ã«ï¼‰
                    unique_key = f"{date_str}_{game_num}"
                    if unique_key in processed_dates:
                        continue
                    processed_dates.add(unique_key)

                    rank_items = col.find_all('div', class_='p-gamesResult__rank-item')
                    for item in rank_items:
                        rank = item.find('div', class_='p-gamesResult__rank-badge').get_text(strip=True)
                        player = item.find('div', class_='p-gamesResult__name').get_text(strip=True).replace(" ", "").replace("ã€€", "")
                        point = float(item.find('div', class_='p-gamesResult__point').get_text(strip=True).replace('pt', '').replace('â–²', '-').replace(',', ''))
                        all_games.append({"date": date_str, "game_count": game_num, "rank": int(rank), "player": player, "point": point})
                        count += 1
            except: continue
        return count

    # æœ€æ–°ãƒšãƒ¼ã‚¸ã®è§£æ
    print("  æœ€æ–°ãƒšãƒ¼ã‚¸ã‚’è§£æä¸­...")
    extract_from_soup(soup)

    # â˜…ã“ã“ãŒæ–°æ©Ÿèƒ½ï¼š2024å¹´ä»¥å‰ã®ãƒ‡ãƒ¼ã‚¿ãªã©ã€éå»ãƒ‡ãƒ¼ã‚¿ãŒåˆ¥URLã«ã‚ã‚‹å ´åˆã®å¯¾å¿œ
    # ï¼ˆå…¬å¼ã‚µã‚¤ãƒˆã®æ§‹é€ ã«ã‚ˆã‚Šã¾ã™ãŒã€å¿µã®ãŸã‚è€ƒãˆã‚‰ã‚Œã‚‹ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–URLã‚‚è¦‹ã«è¡Œãã¾ã™ï¼‰
    # å¿…è¦ã§ã‚ã‚Œã°ã€ã“ã“ã« url_list = ["https://m-league.jp/games/2024/"] ãªã©ã‚’è¿½åŠ ã§ãã¾ã™
    
    if all_games:
        df = pd.DataFrame(all_games)
        # æ—¥ä»˜ã§ä¸¦ã³æ›¿ãˆï¼ˆå¤ã„é †ã«ã—ã¦ãŠãã¨ã‚°ãƒ©ãƒ•ãŒè¦‹ã‚„ã™ã„ï¼‰
        df = df.sort_values(by=['date', 'game_count'])
        df.to_sql('games', conn, if_exists='replace', index=False)
        print(f"âœ… è©¦åˆçµæœ: {len(df)} ä»¶ä¿å­˜å®Œäº†")
        print(f"   ğŸ“… ãƒ‡ãƒ¼ã‚¿æœŸé–“: {df['date'].min()} ã€œ {df['date'].max()}")
    else:
        print("âš ï¸ è©¦åˆçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# -------------------------------------------------------
# 3. å€‹äººæˆç¸¾ (stats)
# -------------------------------------------------------
def scrape_stats(conn):
    soup = get_soup("https://m-league.jp/stats/")
    if not soup: return
    data_list = []
    sections = soup.find_all('section', class_='p-stats__team')
    for section in sections:
        try:
            team_name = section.find('h2', class_='p-stats__teamName').get_text(strip=True)
            table = section.find('table', class_='p-stats__table')
            if not table: continue
            rows = table.find_all('tr')
            players = [p.get_text(strip=True).replace(" ", "").replace("ã€€", "") for p in rows[0].find_all('th')[1:]]
            player_stats = {p: {'team': team_name, 'player': p} for p in players}
            key_map = {'è©¦åˆæ•°': 'matches', 'ç·å±€æ•°': 'total_hands', 'ãƒã‚¤ãƒ³ãƒˆ': 'points', 'å¹³ç€': 'avg_rank', '1ä½': 'rank_1_count', '2ä½': 'rank_2_count', '3ä½': 'rank_3_count', '4ä½': 'rank_4_count', 'ãƒˆãƒƒãƒ—ç‡': 'top_rate', 'é€£å¯¾ç‡': 'rentai_rate', 'ãƒ©ã‚¹å›é¿ç‡': 'last_avoid_rate', 'ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢': 'best_score', 'å¹³å‡æ‰“ç‚¹': 'avg_score', 'å‰¯éœ²ç‡': 'furo_rate', 'ãƒªãƒ¼ãƒç‡': 'riichi_rate', 'ã‚¢ã‚¬ãƒªç‡': 'agari_rate', 'æ”¾éŠƒç‡': 'hoju_rate', 'æ”¾éŠƒå¹³å‡æ‰“ç‚¹': 'hoju_avg_score'}
            for row in rows[1:]:
                header = row.find('th').get_text(strip=True)
                if header in key_map:
                    db_key = key_map[header]
                    cols = row.find_all('td')
                    for i, col in enumerate(cols):
                        if i < len(players):
                            val = col.get_text(strip=True)
                            try: val = float(val) if '.' in val else int(val)
                            except: pass
                            player_stats[players[i]][db_key] = val
            data_list.extend(player_stats.values())
        except: continue
    if data_list:
        df = pd.DataFrame(data_list)
        df.to_sql('stats', conn, if_exists='replace', index=False)
        print(f"âœ… å€‹äººã‚¹ã‚¿ãƒƒãƒ„: {len(df)} å")

if __name__ == "__main__":
    conn = sqlite3.connect(DB_NAME)
    print("--- ãƒ‡ãƒ¼ã‚¿å…¨å›åé–‹å§‹ ---")
    scrape_points(conn)
    scrape_games(conn)
    scrape_stats(conn)
    conn.close()
    print("--- å®Œäº† ---")